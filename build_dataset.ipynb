{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "import geopandas as gpd\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_dir = 'data/info'\n",
    "status_dir = 'data/status'\n",
    "free_bike_dir = 'data/free_bike'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_files = [file_name for file_name in os.listdir(stations_dir)]\n",
    "status_files = [file_name for file_name in os.listdir(status_dir)]\n",
    "free_bike_files = [file_name for file_name in os.listdir(free_bike_dir)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_data_list = []\n",
    "\n",
    "for file in tqdm(stations_files, desc='file'):\n",
    "    \n",
    "    with open(f'data/info/{file}', 'r') as file_object:\n",
    "        try:\n",
    "            stations_json_load = json.load(file_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'error on {file}: {e}')\n",
    "            continue\n",
    "\n",
    "    time_stations_data = (\n",
    "        pd.json_normalize(\n",
    "            data=stations_json_load,\n",
    "            record_path= [\n",
    "                ['data','stations']\n",
    "            ],\n",
    "            meta='last_updated',\n",
    "        )\n",
    "        .filter(items=[\n",
    "            'last_updated',\n",
    "            'station_id',\n",
    "            'short_name',\n",
    "            'name',\n",
    "            'capacity',\n",
    "            'lat',\n",
    "            'lon'\n",
    "        ])\n",
    "    )\n",
    "\n",
    "    file_timestamp = int(re.search('\\d*',file).group())\n",
    "\n",
    "    time_stations_data['status_last_updated_fetched_timestamp'] = file_timestamp \n",
    "\n",
    "\n",
    "    stations_data_list.append(time_stations_data)\n",
    "\n",
    "stations_data = pd.concat(stations_data_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_data_list = []\n",
    "\n",
    "for file in tqdm(status_files):\n",
    "\n",
    "    with open(f'data/status/{file}', 'r') as file_object:\n",
    "        try:\n",
    "            status_json_load = json.load(file_object)\n",
    "        except json.JSONDecodeError as e:\n",
    "            print(f'error on {file}: {e}')\n",
    "            continue\n",
    "\n",
    "    time_status_data = pd.json_normalize(\n",
    "        data=status_json_load,\n",
    "        record_path= [\n",
    "            ['data','stations']\n",
    "        ],\n",
    "        meta='last_updated',\n",
    "    ).filter(items=[\n",
    "        'last_updated',\n",
    "        'station_id',\n",
    "        'station_status',\n",
    "        'is_renting',\n",
    "        'is_returning',\n",
    "        'num_docks_available',\n",
    "        'num_bikes_available',\n",
    "        'num_ebikes_available',\n",
    "        'num_bikes_disabled',\n",
    "        'num_docks_disabled',\n",
    "        'num_ebikes_disabled',\n",
    "        'valet.active'\n",
    "    ])\n",
    "\n",
    "    status_data_list.append(time_status_data)\n",
    "\n",
    "\n",
    "status_data = pd.concat(status_data_list)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "localize times"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "status_data['last_updated'] = (\n",
    "    status_data['last_updated']\n",
    "    .apply(pd.Timestamp, unit='s', tz='America/New_York')\n",
    ")\n",
    "\n",
    "stations_data['status_last_updated_fetched_timestamp'] = (\n",
    "    stations_data['status_last_updated_fetched_timestamp']\n",
    "    .apply(pd.Timestamp, unit='s', tz='America/New_York')\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that stations data timestamps have matching status data timestamps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_data['station_id'] = stations_data['station_id'].astype(str)\n",
    "status_data['station_id'] = status_data['station_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert stations_data['status_last_updated_fetched_timestamp'].isin(status_data['last_updated']).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    stations_data\n",
    "    .merge(\n",
    "        status_data, \n",
    "        left_on=['status_last_updated_fetched_timestamp','station_id'],\n",
    "        right_on=['last_updated','station_id'],\n",
    "        how='inner',\n",
    "        suffixes=['_stations',None]\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "check that each station_id is a unique physical location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "assert dataset.groupby('station_id')['lat'].nunique().max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "stations_locations = (\n",
    "    dataset\n",
    "    .drop_duplicates(subset='station_id')\n",
    "    .set_index('station_id')\n",
    "    [['lat','lon']]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = (\n",
    "    dataset\n",
    "    .set_index(['last_updated','station_id'])\n",
    "    .drop(columns=[\n",
    "        'name',\n",
    "        'lat','lon',\n",
    "        'status_last_updated_fetched_timestamp',\n",
    "        'last_updated_stations',\n",
    "        ])\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "save out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save dataset as CSV\n",
    "dataset.to_csv('station_status.csv', index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# # Load your local dataset (replace 'station_status.csv' with the path to your dataset)\n",
    "df = pd.read_csv(\"station_status.csv\")\n",
    "df.dtypes"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
